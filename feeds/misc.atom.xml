<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Felix Martinez | Data Engineering Blog - misc</title><link href="http://localhost:8000/" rel="alternate"></link><link href="http://localhost:8000/feeds/misc.atom.xml" rel="self"></link><id>http://localhost:8000/</id><updated>2020-01-12T00:30:00-06:00</updated><subtitle>My personal toolbox</subtitle><entry><title>Docker for data engineering, Part 1</title><link href="http://localhost:8000/2020/01/docker-containers.html" rel="alternate"></link><published>2020-01-12T00:30:00-06:00</published><updated>2020-01-12T00:30:00-06:00</updated><author><name>Felix Martinez</name></author><id>tag:localhost,2020-01-12:/2020/01/docker-containers.html</id><summary type="html">&lt;p&gt;In this post I'll list the most common containers that I'm using in my
daily work. I do not pretend to explain the theory behind docker's containers
instead you can get some useful docker files to work with.&lt;/p&gt;
&lt;p&gt;Depending on the scenario that you are facing up you can do …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In this post I'll list the most common containers that I'm using in my
daily work. I do not pretend to explain the theory behind docker's containers
instead you can get some useful docker files to work with.&lt;/p&gt;
&lt;p&gt;Depending on the scenario that you are facing up you can do a combination
with them to get the environment according with your requirements.&lt;/p&gt;
&lt;div class="section" id="databases"&gt;
&lt;h2&gt;Databases&lt;/h2&gt;
&lt;p&gt;Don't forget setting up the volumes correctly when you are working with
databases this features prevent the loss of your data when the container
no longer exists.&lt;/p&gt;
&lt;div class="section" id="mongo"&gt;
&lt;h3&gt;Mongo&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;docker run --name mongo_nea -d &lt;span class="se"&gt;\&lt;/span&gt;
-v &lt;span class="nv"&gt;$HOME&lt;/span&gt;/data/mongo:/app/data &lt;span class="se"&gt;\&lt;/span&gt;
-v &lt;span class="nv"&gt;$HOME&lt;/span&gt;/config/mongod.conf:/etc/mongo.conf &lt;span class="se"&gt;\&lt;/span&gt;
-p &lt;span class="m"&gt;27017&lt;/span&gt;:27017 mongo:3.6.2 &lt;span class="se"&gt;\&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="influx"&gt;
&lt;h3&gt;Influx&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;docker run --name influxdb -d &lt;span class="se"&gt;\&lt;/span&gt;
-p &lt;span class="m"&gt;8083&lt;/span&gt;:8083 -p &lt;span class="m"&gt;8086&lt;/span&gt;:8086
-v /local/path/to/db:/var/lib/influxdb
-v /local/path/to/config:/etc/influxdb/influxdb.conf:ro &lt;span class="se"&gt;\&lt;/span&gt;
influxdb:1.2 -config /etc/influxdb/influxdb.conf
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="postgresql"&gt;
&lt;h3&gt;PostgreSQL&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;docker run -d --name this_postgres -v pg-datastore:/var/lib/postgresql/data -p &lt;span class="m"&gt;5432&lt;/span&gt;:5432 postgres
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="mysql"&gt;
&lt;h3&gt;MySQL&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;docker run -v &lt;span class="nv"&gt;$HOME&lt;/span&gt;/data/mysql:/var/lib/mysql &lt;span class="se"&gt;\&lt;/span&gt;
--name mysql_spring_boot -e &lt;span class="nv"&gt;MYSQL_DATABASE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;test&amp;#39;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
-e &lt;span class="nv"&gt;MYSQL_USER&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;mysql&amp;#39;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
-e &lt;span class="nv"&gt;MYSQL_PASSWORD&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;mysql&amp;#39;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
-e &lt;span class="nv"&gt;MYSQL_ROOT_PASSWORD&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;admin&amp;#39;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
-e &lt;span class="nv"&gt;MYSQL_ALLOW_EMPTY_PASSWORD&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;yes&amp;#39;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
-d mysql
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="sql-server"&gt;
&lt;h3&gt;SQL Server&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;docker run -e &lt;span class="s1"&gt;&amp;#39;ACCEPT_EULA=Y&amp;#39;&lt;/span&gt; -e &lt;span class="s1"&gt;&amp;#39;SA_PASSWORD=secret&amp;#39;&lt;/span&gt; -p &lt;span class="m"&gt;1433&lt;/span&gt;:1433 &lt;span class="se"&gt;\&lt;/span&gt;
-d mcr.microsoft.com/mssql/server:2017-latest
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="data-analysis"&gt;
&lt;h2&gt;Data Analysis&lt;/h2&gt;
&lt;p&gt;The interesting part here is &lt;tt class="docutils literal"&gt;pwd&lt;/tt&gt; wich is the current path, and
will be linked with the workpace in jupyter. So for example when I
receive a new dataset sample I run the container from the unzipped
folder and get access to the entire dataset from jupyter for analyze
it with pandas.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;docker run -d -v &lt;span class="sb"&gt;`&lt;/span&gt;&lt;span class="nb"&gt;pwd&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;:/home/jovyan/data &lt;span class="se"&gt;\&lt;/span&gt;
-P jupyter/scipy-notebook
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="data-pipelines"&gt;
&lt;h2&gt;Data Pipelines&lt;/h2&gt;
&lt;div class="section" id="kafka"&gt;
&lt;h3&gt;Kafka&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;docker run -d --name zookeeper -p &lt;span class="m"&gt;2181&lt;/span&gt;:2181 dockerkafka/zookeeper
docker run --name kafka -p &lt;span class="m"&gt;9092&lt;/span&gt;:9092 --link zookeeper:zookeeper dockerkafka/kafka
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;// Get the IPs
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;ZK_IP&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;sudo docker inspect --format &lt;span class="s1"&gt;&amp;#39;{{ .NetworkSettings.IPAddress }}&amp;#39;&lt;/span&gt; zookeeper&lt;span class="k"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;KAFKA_IP&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;sudo docker inspect --format &lt;span class="s1"&gt;&amp;#39;{{ .NetworkSettings.IPAddress }}&amp;#39;&lt;/span&gt; kafka&lt;span class="k"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo docker &lt;span class="nb"&gt;exec&lt;/span&gt; -it kafka bash
kafka-topics.sh --create --topic &lt;span class="nb"&gt;test&lt;/span&gt; --zookeeper &lt;span class="nv"&gt;$ZK_IP&lt;/span&gt;:2181 --replication-factor &lt;span class="m"&gt;1&lt;/span&gt; --partitions &lt;span class="m"&gt;1&lt;/span&gt;
kafka-console-producer.sh --topic &lt;span class="nb"&gt;test&lt;/span&gt; --broker-list &lt;span class="nv"&gt;$KAFKA_IP&lt;/span&gt;:9092

sudo docker &lt;span class="nb"&gt;exec&lt;/span&gt; -it kafka bash
kafka-console-consumer.sh --topic &lt;span class="nb"&gt;test&lt;/span&gt; --from-beginning --zookeeper &lt;span class="nv"&gt;$ZK_IP&lt;/span&gt;:2181
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="apache-nifi"&gt;
&lt;h3&gt;Apache NiFi&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;docker run --name nifi &lt;span class="se"&gt;\&lt;/span&gt;
-v &lt;span class="nv"&gt;$HOME&lt;/span&gt;/data/nifi/sample.nar:/opt/nifi/nifi-1.7.0/lib/sample.nar &lt;span class="se"&gt;\&lt;/span&gt;
-p &lt;span class="m"&gt;8081&lt;/span&gt;:8080 -d apache/nifi:1.7.0
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Docker"></category></entry><entry><title>Migrating from MongoDB to InfluxDB</title><link href="http://localhost:8000/2018/07/influxdb.html" rel="alternate"></link><published>2018-07-11T00:45:00-05:00</published><updated>2018-07-11T00:45:00-05:00</updated><author><name>Felix Martinez</name></author><id>tag:localhost,2018-07-11:/2018/07/influxdb.html</id><summary type="html">&lt;p&gt;One of my first tasks in my job was to migrate from MongoDB to InfluxDB.
In this post, I'm going to summarize my experience in such an effort and how
I did deal with InfluxDB with no previous experience.&lt;/p&gt;
&lt;div class="section" id="the-scenario"&gt;
&lt;h2&gt;The Scenario&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;We are parsing counters from monitoring systems who come …&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;One of my first tasks in my job was to migrate from MongoDB to InfluxDB.
In this post, I'm going to summarize my experience in such an effort and how
I did deal with InfluxDB with no previous experience.&lt;/p&gt;
&lt;div class="section" id="the-scenario"&gt;
&lt;h2&gt;The Scenario&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;We are parsing counters from monitoring systems who come in binary and XML files. These
files contain lots of metrics however not all are useful for later analysis. The filter
rules are stored in configuration files along with the solution.&lt;/li&gt;
&lt;li&gt;The parsers are writing single documents in MongoDB using simple insertion operations.
Parsing 250 Gb of the sample data takes around a week running in a virtual machine with
16Gb of memory, 8 processors.&lt;/li&gt;
&lt;li&gt;The parsers are written in Java using multithreading and other APIs like SAX for XML reads,
native MongoDB client, logging, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="data-modeling"&gt;
&lt;h2&gt;Data modeling&lt;/h2&gt;
&lt;p&gt;Whereas while the objective of the re-architecting is improving the performance of the data
acquisition is absolutely imperative to change the data modeling not just for performance
purposes but also for time series analysis.&lt;/p&gt;
&lt;p&gt;Consider the documents written in Mongo like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nx"&gt;_id&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;UUID&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
    &lt;span class="nx"&gt;name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;counter_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nx"&gt;date&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;$date&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;2017-08-11T17:54:14.692Z&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="nx"&gt;prop1&lt;/span&gt;&lt;span class="p"&gt;,...,&lt;/span&gt;&lt;span class="nx"&gt;propn&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The properties &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;prop1,...,propn&lt;/span&gt;&lt;/tt&gt; are numeric values or numeric arrays in some cases.
The hardest work was reviewing all those counters and arranged into useful and non required
by the algorithms.&lt;/p&gt;
&lt;p&gt;In MongoDB, you can model as embedded document such kind of structure easily, and using a
bulk operation you can improve the data load. In InfluxDB, however, I would not recommend
that design, the data points have a structure like:&lt;/p&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;&amp;lt;measurement&amp;gt;[,&amp;lt;tag-key&amp;gt;=&amp;lt;tag-value&amp;gt;...]&lt;/span&gt; &lt;span class="pre"&gt;&amp;lt;field-key&amp;gt;=&amp;lt;field-value&amp;gt;[,&amp;lt;field2-key&amp;gt;=&amp;lt;field2-value&amp;gt;...]&lt;/span&gt; &lt;span class="pre"&gt;[unix-nano-timestamp]&lt;/span&gt;&lt;/tt&gt;&lt;/p&gt;
&lt;p&gt;Where tags are indexed and field are not, considering this you need to avoid writing
lots of tags for querying and lots of fields as properties because it has an elevating cost.
It is better instead write consistent metrics within the tagging domain and use different
measurements rather than a huge structure. There can be many options for downsampling
specific measurements using aggregation pipelines for future analysis.
You can learn more about InfluxDB's concepts &lt;a class="reference external" href="https://docs.influxdata.com/influxdb/v1.8/concepts/key_concepts/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let's jump into an example to be more clear. Looking into the UCI Machine Learning Repository
I found &lt;a class="reference external" href="http://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data"&gt;this&lt;/a&gt; data set which includes hourly air pollutants data from Beijing.
It looks interesting because contains multiple files with well-formed rows.&lt;/p&gt;
&lt;p&gt;From row 2 to 5 will represent the timestamp in any case. Let's suppose that we want to plot
the wind, for this requirement would be useful to get a measurement structure taking the wind
speed and direction, row 17 and 16 respectively. Our measurement's schema will be like:
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;&amp;quot;Wind,station=%s&lt;/span&gt; &lt;span class="pre"&gt;direction=\&amp;quot;%s\&amp;quot;,speed=%s&lt;/span&gt; %d&amp;quot;&lt;/tt&gt;&lt;/p&gt;
&lt;p&gt;It will allow us queries like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;select&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="k"&gt;from&lt;/span&gt; &lt;span class="n"&gt;Wind&lt;/span&gt; &lt;span class="k"&gt;where&lt;/span&gt; &lt;span class="n"&gt;station&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Aotizhongxin&amp;#39;&lt;/span&gt; &lt;span class="k"&gt;limit&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Depends on the amount of data expected and the retention policies would be better
to have measurements like:&lt;/p&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;Aotizhongxin_PM2_SO2&lt;/tt&gt;
&lt;tt class="docutils literal"&gt;Aotizhongxin_TEMP&lt;/tt&gt;&lt;/p&gt;
&lt;p&gt;Whatever would be the analysis goals you can decompose the data as get the features required.
In the given example I do suppose that is required to correlate PM2.5 with SO2 concentrations
and other measurements for enrichment, I'm no an expert in air pollution it's clear, but the
fact is that you need to develop ideas about what kind of measurements would be useful for
labeling or predicting. This may be addressed in collaboration with the Data Science team.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-code"&gt;
&lt;h2&gt;The Code&lt;/h2&gt;
&lt;p&gt;There is nothing special in &lt;a href="https://github.com/feeliks-ahumada/influxmigration" target="_blank"&gt;this code&lt;/a&gt;, we are reading the files using multithreading,
and storing using the latest Java's API version, but I'm still using the InfluxDB 1.8.
I must say however the API has good improvements in reads and writes.&lt;/p&gt;
&lt;p&gt;As I mention previously when we depicted data points a good practice is to handle separated
measurements. In the next code, we are formatting the Wind measurement:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;static&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="nf"&gt;lineProtocol&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;[]&lt;/span&gt; &lt;span class="n"&gt;cols&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;\&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="na"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;,&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

    &lt;span class="n"&gt;ZonedDateTime&lt;/span&gt; &lt;span class="n"&gt;timestamp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;getZonedDateTime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cols&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

    &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;try&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;format&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Wind,station=%s direction=\&amp;quot;%s\&amp;quot;,speed=%s %d&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;cols&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;17&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;cols&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;cols&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;timestamp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;toEpochSecond&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;catch&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Exception&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;format&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;fails,m=%s value=%s %d&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;cols&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;17&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;cols&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;timestamp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;toEpochSecond&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
        &lt;span class="n"&gt;logger&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;getMessage&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Notice that this method is a good candidate for re-engineering, to create a new class using
the factory patterns to get a custom measurement format depending on the feature that we
are looking for.&lt;/p&gt;
&lt;p&gt;We now have the next sampled series:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;show&lt;/span&gt; &lt;span class="n"&gt;series&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Key
---
Wind,station&lt;span class="o"&gt;=&lt;/span&gt;Aotizhongxin
Wind,station&lt;span class="o"&gt;=&lt;/span&gt;Changping
Wind,station&lt;span class="o"&gt;=&lt;/span&gt;Dingling
Wind,station&lt;span class="o"&gt;=&lt;/span&gt;Dongsi
Wind,station&lt;span class="o"&gt;=&lt;/span&gt;Guanyuan
Wind,station&lt;span class="o"&gt;=&lt;/span&gt;Gucheng
Wind,station&lt;span class="o"&gt;=&lt;/span&gt;Huairou
Wind,station&lt;span class="o"&gt;=&lt;/span&gt;Nongzhanguan
Wind,station&lt;span class="o"&gt;=&lt;/span&gt;Shunyi
Wind,station&lt;span class="o"&gt;=&lt;/span&gt;Tiantan
Wind,station&lt;span class="o"&gt;=&lt;/span&gt;Wanliu
Wind,station&lt;span class="o"&gt;=&lt;/span&gt;Wanshouxigong
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To compile and run the code you need to install Java 8, Maven, and running properly the InfluxDB,
I recommend the &lt;a href="https://github.com/influxdata/sandbox" target="_blank"&gt;sandbox&lt;/a&gt; for testing purposes.&lt;/p&gt;
&lt;blockquote&gt;
Post migrated from a previous blog, updated with source code.&lt;/blockquote&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="NewSQL"></category><category term="InfluxDB"></category></entry></feed>